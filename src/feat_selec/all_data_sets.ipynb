{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"../\"\n",
    "%pwd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io \n",
    "import pandas as pd  \n",
    "import pyreadr\n",
    "\n",
    "from load_data import *\n",
    "from pred_score import *\n",
    "from Filter_FS import *\n",
    "from hybrid_FS import *\n",
    "from overlap_genes import *\n",
    "from crossValidation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing seed to get reproducible results\n",
    "random.seed(3)\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with preselected genes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load preprocess data\n",
    "all_norm= np.array(pd.read_csv ('../data/processed_data/all.csv'))\n",
    "y = np.array(all_norm[:,-1],dtype=int)\n",
    "all_norm = all_norm[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict and evaluate\n",
    "model = FamiliesClusters(np.unique(y),compute_precision,True)\n",
    "pred = model.fit_predict(all_norm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_norm.shape, len(pred))\n",
    "acc = model.score_\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information maximizer (MIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for MIM method\n",
    "N = np.arange(80,1500,30)\n",
    "kwargs = {'Model': FamiliesClusters, 'Scoring': compute_precision,'maximize': True,'N': N, 'n_neighbors': 3, 'plot': True} \n",
    "\n",
    "subset, score_training, score_testing = cross_validation(y,all_norm, FamiliesClusters, compute_precision,True, 5,  MIM, **kwargs)\n",
    "\n",
    "mean_score_test, std_score_test = np.mean(score_testing), np.std(score_testing)    \n",
    "print('test', mean_score_test, std_score_test)\n",
    "\n",
    "#Predict and evaluates on whole data  set\n",
    "model = FamiliesClusters(np.unique(y),compute_precision,True)\n",
    "x_subset = all_norm[:, subset]\n",
    "pred = model.fit_predict(x_subset,y)\n",
    "\n",
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPqyZ49FMaxj"
   },
   "source": [
    "# Anova F test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for ANOVA method\n",
    "N = np.arange(80,3000,30)\n",
    "kwargs = {'Model': FamiliesClusters, 'Scoring': compute_precision,'maximize': True,'N': N, 'plot': True} \n",
    "\n",
    "subset, score_training, score_testing = cross_validation(y,all_norm, FamiliesClusters, compute_precision,True, 5,  ANOVA, **kwargs)\n",
    "\n",
    "mean_score_test, std_score_test = np.mean(score_testing), np.std(score_testing)    \n",
    "print('test', mean_score_test, std_score_test)\n",
    "\n",
    "#Predict and evaluate on whole data  set\n",
    "model = FamiliesClusters(np.unique(y),compute_precision,True)\n",
    "x_subset = all_norm[:, subset]\n",
    "pred = model.fit_predict(x_subset,y)\n",
    "\n",
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information and stimulated annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for MI + stimulated annealing method\n",
    "N = np.array([700])\n",
    "kwargs = {'Model': FamiliesClusters, 'Scoring': compute_precision,'maximize': True,'N': N, 'n_iter': 1400, 'n_neighbors': 3, 'p_mutate': 0.1, 'c': 1, 'plot': True} \n",
    "\n",
    "subset, score_training, score_testing = cross_validation(y,all_norm, FamiliesClusters, compute_precision,True, 5,  MI_stimulated_annealing, **kwargs)\n",
    "\n",
    "mean_score_test, std_score_test = np.mean(score_testing), np.std(score_testing)    \n",
    "print('test', mean_score_test, std_score_test)\n",
    "\n",
    "#Predict and evaluate on whole data  set\n",
    "model = FamiliesClusters(np.unique(y),compute_precision,True)\n",
    "x_subset = all_norm[:, subset]\n",
    "pred = model.fit_predict(x_subset,y)\n",
    "\n",
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every data set but switch which is used for training and which is used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTestAll(y:np.array, x:np.array, ind_dataset:list, i:int):\n",
    "    \"\"\"Split the merged data, one data set is keept for testing, the others for training.\n",
    "  \n",
    "      parameters:\n",
    "      y : np.array,\n",
    "        family of each data points\n",
    "      x : np.array,\n",
    "        features of each data points\n",
    "      ind_dataset : list,\n",
    "        list of indices where each data set is stored\n",
    "      i: int, \n",
    "        ind of the data set to keep for testing\n",
    "        \n",
    "\n",
    "      returns:\n",
    "      x_train : np.array,\n",
    "        norm data without the test dataset for training\n",
    "      y_train : np.array,\n",
    "        families of each data point without the test dataset for training\n",
    "      x_test : np.array,\n",
    "        norm data of the test dataset\n",
    "      y_test : np.array,\n",
    "        families of each data point of the test dataset\"\"\"\n",
    "    \n",
    "    ind_i = None\n",
    "    if i == 0:\n",
    "        ind_i = np.arange(0,ind_dataset[i],1)\n",
    "    else:\n",
    "        ind_i = np.arange(ind_dataset[i-1], ind_dataset[i], 1)\n",
    "    \n",
    "    \n",
    "    x_train = np.delete(x, ind_i, axis=0)\n",
    "    y_train = np.delete(y, ind_i)\n",
    "    x_test = x[ind_i,:]\n",
    "    y_test = y[ind_i]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def optimization_on_allsets(y:np.array, x:np.array, ind_dataset:list, Model_test: Callable, Scoring_test: Callable, maximize_test:bool, kfold:int, func: Callable, **kwargs: dict):\n",
    "    \"\"\" \n",
    "  \n",
    "      parameters:\n",
    "      y : np.array,\n",
    "        family of each data points\n",
    "      x : np.array,\n",
    "        features of each data points\n",
    "      ind_dataset : list,\n",
    "        list of indices where each data set is stored\n",
    "      Model_test : Callable,\n",
    "        the model is fitted using this method\n",
    "      Scoring_test: Callable,\n",
    "        scoring function use to evaluate the model\n",
    "      maximize_test: bool,\n",
    "        if True the scoring function is maximize, else it is minimize\n",
    "      kfold: int,\n",
    "        number of folds for CV\n",
    "      func: Callable,\n",
    "        feature selection function, should return seleted subset and associated score\n",
    "      kwargs: **kwargs : dict,\n",
    "        dictionnary of parameters and their values (kwargs = {'param_name' : value}) to pass to the given method (func)\n",
    "        \n",
    "\n",
    "      returns:\n",
    "      final_subset : np.array,\n",
    "        subset of features with the best score\n",
    "      best_test_score : float,\n",
    "        test score obtained with the best subset of features \"\"\"\n",
    "    \n",
    "    #Store score training and best subset\n",
    "    score_training = []\n",
    "    score_testing = []\n",
    "    final_subset = []\n",
    "    \n",
    "    for i in range(0,len(ind_dataset)):\n",
    "        #Get split data\n",
    "        x_train, y_train, x_test, y_test = getTrainTestAll(y, x, ind_dataset, i)\n",
    "        \n",
    "        #Run feature selection on training set\n",
    "        subset, score = func(y_train, x_train, **kwargs)\n",
    "        \n",
    "        #Evaluate subset on test set\n",
    "        model_test = Model_test(np.unique(y_test),Scoring_test,True)\n",
    "        pred_test = model_test.fit_predict(x_test[:, subset],y_test)\n",
    "        test_score = model_test.score(x_test[:, subset],y_test)\n",
    "        \n",
    "        #Store best score on current folds\n",
    "        score_training.append(score)\n",
    "        score_testing.append(test_score)\n",
    "        if (len(final_subset) == 0 or np.argmax(score_testing) == i): #if the last best test score is best overall keep subset as the finals subset\n",
    "            final_subset = subset\n",
    "        \n",
    "    return final_subset, score_training, score_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_dataset = np.array(pd.read_csv ('../data/processed_data/ind_datasets.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information maximizer (MIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for MIM method\n",
    "N = np.arange(80,1500,30)\n",
    "kwargs = {'Model': FamiliesClusters, 'Scoring': compute_precision,'maximize': True,'N': N, 'n_neighbors': 3, 'plot': True} \n",
    "\n",
    "subset, score_training, score_testing = cross_validation(y,all_norm, ind_dataset, FamiliesClusters, compute_precision,True, 5,  MIM, **kwargs)\n",
    "\n",
    "mean_score_test, std_score_test = np.mean(score_testing), np.std(score_testing)    \n",
    "print('test', mean_score_test, std_score_test)\n",
    "\n",
    "#Predict and evaluates on whole data  set\n",
    "model = FamiliesClusters(np.unique(y),compute_precision,True)\n",
    "x_subset = all_norm[:, subset]\n",
    "pred = model.fit_predict(x_subset,y)\n",
    "\n",
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
