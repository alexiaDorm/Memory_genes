{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all necessary libraries and .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  \n",
    "import math\n",
    "\n",
    "from load_data import *\n",
    "from pred_score import *\n",
    "from Filter_FS import *\n",
    "from overlap_genes import *\n",
    "from crossValidation import *\n",
    "from evaluation_measure import *\n",
    "\n",
    "#Fixing seed to get reproducible results\n",
    "random.seed(3)\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load normalized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the normalized data should be: number of cells x number of genes.  (333, 11895)\n"
     ]
    }
   ],
   "source": [
    "#Enter filepath bellow as a str\n",
    "filepath = '../data/processed_data/AE3.csv' #Provide the path to the normalized data you would like to predict the family of.\n",
    "\n",
    "norm = pd.read_csv(filepath)#.T\n",
    "genes_names = norm.columns\n",
    "norm = np.array(norm) #convert to array\n",
    "#If that's not case can just comment/uncomment the .T\n",
    "print('Shape of the normalized data should be: number of cells x number of genes. ', norm.shape)\n",
    "\n",
    "#If the optimization takes too much time, you might consider removing all genes that are not expressed in at least 5% of the cells.\n",
    "#To do so, uncomment the lines bellow\n",
    "gene_expressed = filter_norm_data(norm.T, 0.05)\n",
    "norm = (norm.T[gene_expressed]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with all genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When clustering the data in an unsupervised manner, the maximum expected number of cell in one family/cluster needs to be provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cophenetic coefficient and recovery:  0.22877350018532344   0.6726726726726727\n"
     ]
    }
   ],
   "source": [
    "Nmax = 3 #Put the max number of cell expected in a cluster\n",
    "\n",
    "#Predict families of the dataset and evaluate using Cophenetic coefficient\n",
    "model = FamiliesClusters(family_interest_ = None, Scoring_ = compute_cophe_coeff, maximize_ = True)\n",
    "pred = model.fit_predict(X=norm, y=None, NmaxCluster=Nmax) \n",
    "\n",
    "print('Cophenetic coefficient and recovery: ', model.score_, ' ', model.recovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information maximizer (MIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information maximization (MIM) utilizes the mutual information. It is an estimate of the statistical dependency between random variables. It is able to capture non-linear dependencies.\n",
    "$$ MI = I(X ; Y) = H(X) â€“ H(X | Y) $$\n",
    "$$ H(X) = - \\sum_{i=1}^{n} P(x_i)log_2(P(x_i))$$\n",
    "The simplest way of using this scoring criterion is by ranking the features with decreasing mutual information content and choosing the top N features.\n",
    "\n",
    "A grid search is perform to find the final size of the gene subset. You might want to play around with the range of final number of genes depending on the size of the dataset. The number of fold for k-fold cross-validation also need to be provided. For MIM fature selection, the number of neighbors for the computatation of the mutual information of each gene is a tunable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_cv_unsupervised(x:np.array, kfold:int):\n",
    "    \"\"\" Split the data for CV.\n",
    "  \n",
    "      parameters:\n",
    "      x: np.array,\n",
    "        gene expression of each data points\n",
    "      kfold: int,\n",
    "        number of fold for CV    \n",
    "\n",
    "      returns:\n",
    "      split_x : list of np.array,\n",
    "        split normalized data, split_x[k] = kth split\n",
    "      \"\"\"\n",
    "    \n",
    "    #Get the ind of the cells in each fold\n",
    "    Ncells_fold = math.floor(x.shape[0]/kfold)\n",
    "    cells_ind = np.arange(0,x.shape[0])\n",
    "    ind_folds = list(np.random.choice(cells_ind,(kfold-1, Ncells_fold), replace=False))\n",
    "    \n",
    "    last_fold = list(set(cells_ind) - set(ind_folds[0]))\n",
    "    for i in range (1,len(ind_folds)):\n",
    "        last_fold = list(set(last_fold) - set(ind_folds[i]))\n",
    "    ind_folds.append(np.array(last_fold))\n",
    "    \n",
    "    x_split = []\n",
    "    for fold in ind_folds:\n",
    "        x_split.append(x[fold,:])\n",
    "\n",
    "    return x_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_split = split_data_cv_unsupervised(norm,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x:np.array, Model_test: Callable, Scoring_test: Callable, maximize_test:bool, kfold:int, func: Callable, **kwargs: dict):\n",
    "    \"\"\" Cross validate any feature selection method in a unsupervised manner.\n",
    "  \n",
    "      parameters:\n",
    "      x : np.array,\n",
    "        genes expression of each data points\n",
    "      Model_test : Callable,\n",
    "        the model is fitted using this method\n",
    "      Scoring_test: Callable,\n",
    "        scoring function use to evaluate the model\n",
    "      maximize_test: bool,\n",
    "        if True the scoring function is maximize, else it is minimize\n",
    "      kfold: int,\n",
    "        number of folds for CV\n",
    "      func: Callable,\n",
    "        feature selection function, should return seleted subset and associated score\n",
    "      kwargs: **kwargs : dict,\n",
    "        dictionnary of parameters and their values (kwargs = {'param_name' : value}) to pass to the given method (func)\n",
    "        \n",
    "\n",
    "      returns:\n",
    "      final_subset : np.array,\n",
    "        subset of features with the best score\n",
    "      best_test_score : float,\n",
    "        test score obtained with the best subset of features \"\"\"\n",
    "    \n",
    "    #Store score training and best subset\n",
    "    score_training = []\n",
    "    score_testing = []\n",
    "    final_subset = []\n",
    "    \n",
    "    #Split the data in kfold\n",
    "    split_x = split_data_cv(x,kfold)\n",
    "    \n",
    "    for i in range(0,kfold):\n",
    "        #Get split data\n",
    "        x_test = np.squeeze(split_x[i])\n",
    "        x_train = np.squeeze(split_x[:i] + split_x[i+1:])\n",
    "        \n",
    "        #Run feature selection on training set\n",
    "        subset, score = func(y_train, x_train, **kwargs)\n",
    "        \n",
    "        #Evaluate subset on test set\n",
    "        model_test = Model_test(np.unique(y_test),Scoring_test,True)\n",
    "        pred_test = model_test.fit_predict(x_test[:, subset],y_test)\n",
    "        test_score = model_test.score(x_test[:, subset],y_test)\n",
    "        \n",
    "        #Store best score on current folds\n",
    "        score_training.append(score)\n",
    "        score_testing.append(test_score)\n",
    "        if (len(final_subset) == 0 or np.argmax(score_testing) == i): #if the last best test score is best overall keep subset as the finals subset\n",
    "            final_subset = subset\n",
    "        \n",
    "    return final_subset, score_training, score_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters for MIM method\n",
    "N = np.arange(100,700,300)\n",
    "kwargs = {'Model': FamiliesClusters, 'Scoring': compute_cophe_coeff, 'maximize': True,'N': N, 'n_neighbors': 3, 'plot': True} \n",
    "\n",
    "#Cross validate the MIM optimization \n",
    "subset, score_training, score_testing = cross_validation(y=None, x=norm, Model_test=FamiliesClusters, Scoring_test=compute_cophe_coeff, maximize_test=True, kfold=3,  func=MIM, **kwargs)\n",
    "mean_score_test, std_score_test = np.mean(score_testing), np.std(score_testing)    \n",
    "print('test score and std ', mean_score_test, std_score_test)\n",
    "\n",
    "#Predict and evaluate on whole data  set\n",
    "model = FamiliesClusters(np.unique(y),compute_RI,True)\n",
    "x_subset = AE3[:, subset]\n",
    "pred = model.fit_predict(x_subset,y)\n",
    "\n",
    "print(\"TP, FP, ratio, sensitivity, specificity, precision, NPV, FDR, FNR = \", compute_statTP(y,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
